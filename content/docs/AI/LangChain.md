---
title: LangChain
group: AI
layout: doc
date: 2024-03-22T01:19:04.820Z
tags: [AI]
summary: ç°åœ¨æœ€æµè¡Œçš„æå¤§è¯­è¨€æ¨¡å‹å¼€å‘çš„æ¡†æ¶
sidebar: true
---


## å†™åœ¨å‰é¢

- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰
- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹
- LangChain è¿­ä»£é€Ÿåº¦æ˜æ˜¾å¿«äº Semantic Kernelï¼Œå‡ ä¹æ˜å¤©ä¸€ä¸ªç‰ˆæœ¬
- å­¦ä¹  Langchain è¦å…³æ³¨æ¥å£å˜æ›´


## LangChain çš„æ ¸å¿ƒç»„ä»¶

1. æ¨¡å‹ I/O å°è£…
   - LLMsï¼šå¤§è¯­è¨€æ¨¡å‹
   - Chat Modelsï¼šä¸€èˆ¬åŸºäº LLMsï¼Œä½†æŒ‰å¯¹è¯ç»“æ„é‡æ–°å°è£…
   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿
   - OutputParserï¼šè§£æè¾“å‡º
2. æ•°æ®è¿æ¥å°è£…
   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨
   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc
   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œï¼ˆå•¥æ„æ€ï¼Ÿåˆ«æ€¥ï¼Œåé¢è¯¦ç»†è®²ï¼‰
   - Verctorstores: ï¼ˆé¢å‘æ£€ç´¢çš„ï¼‰å‘é‡çš„å­˜å‚¨
   - Retrievers: å‘é‡çš„æ£€ç´¢
3. è®°å¿†å°è£…
   - Memoryï¼šè¿™é‡Œä¸æ˜¯ç‰©ç†å†…å­˜ï¼Œä»æ–‡æœ¬çš„è§’åº¦ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œä¸Šæ–‡â€ã€â€œå†å²è®°å½•â€æˆ–è€…è¯´â€œè®°å¿†åŠ›â€çš„ç®¡ç†
4. æ¶æ„å°è£…
   - Chainï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ
   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½
     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰
     - Toolkitsï¼šæ“ä½œæŸè½¯ä»¶çš„ä¸€ç»„å·¥å…·é›†ï¼Œä¾‹å¦‚ï¼šæ“ä½œ DBã€æ“ä½œ Gmail ç­‰ç­‰
5. Callbacks

<img src="/images/ai/langchain.png" style="margin-left: 0px" width=500px>


### æ–‡æ¡£ï¼ˆä»¥ Python ç‰ˆä¸ºä¾‹ï¼‰

- åŠŸèƒ½æ¨¡å—ï¼šhttps://python.langchain.com/docs/get_started/introduction
- API æ–‡æ¡£ï¼šhttps://api.python.langchain.com/en/latest/langchain_api_reference.html
- ä¸‰æ–¹ç»„ä»¶é›†æˆï¼šhttps://python.langchain.com/docs/integrations/platforms/
- å®˜æ–¹åº”ç”¨æ¡ˆä¾‹ï¼šhttps://python.langchain.com/docs/use_cases
- è°ƒè¯•éƒ¨ç½²ç­‰æŒ‡å¯¼ï¼šhttps://python.langchain.com/docs/guides/debugging


## ä¸€ã€æ¨¡å‹ I/O å°è£…

æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚

### 1.1 æ¨¡å‹ APIï¼šLLM vs. ChatModel



```python
#å®‰è£…æœ€æ–°ç‰ˆæœ¬
#!pip install --upgrade langchain
#!pip install --upgrade langchain-openai # v0.1.0æ–°å¢çš„åº•åŒ…
```


```python
# ä¸æœ¬è¯¾æ— å…³ï¼Œè¯·å¿½ç•¥
import os
os.environ["LANGCHAIN_PROJECT"] = ""
os.environ["LANGCHAIN_API_KEY"] = ""
os.environ["LANGCHAIN_ENDPOINT"] = ""
os.environ["LANGCHAIN_TRACING_V2"] = ""

OPENAI_API_KEY = "sk-xxxxx"
```

### 1.1.1 OpenAI æ¨¡å‹å°è£…



```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")  # é»˜è®¤æ˜¯gpt-3.5-turbo
response = llm.invoke("ä½ æ˜¯è°")
print(response.content)
```

    æˆ‘æ˜¯ä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜å’Œä¸ä½ è¿›è¡Œäº¤è°ˆã€‚ä½ æœ‰ä»€ä¹ˆæƒ³çŸ¥é“æˆ–æƒ³èŠçš„å—ï¼Ÿ


### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…



```python
from langchain.schema import (
    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role
    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role
    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role
)

messages = [
    SystemMessage(content="ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚"),
    HumanMessage(content="æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«ç‹å“ç„¶ã€‚"),
    AIMessage(content="æ¬¢è¿ï¼"),
    HumanMessage(content="æˆ‘æ˜¯è°")
]

ret = llm.invoke(messages)

print(ret)
```

    content='ä½ æ˜¯æˆ‘ä»¬çš„å­¦å‘˜ï¼Œåå­—å«ç‹å“ç„¶ã€‚' response_metadata={'finish_reason': 'stop', 'logprobs': None}


<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨
</div>


### 1.1.3 æ¢ä¸ªå›½äº§æ¨¡å‹



```python
#!pip install qianfan
```

```python
# å…¶å®ƒæ¨¡å‹åˆ†è£…åœ¨ langchain_community åº•åŒ…ä¸­
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_core.messages import HumanMessage
import os

llm = QianfanChatEndpoint(
    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),
    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')
)

messages = [
    HumanMessage(content="ä½ æ˜¯è°")
]

ret = llm.invoke(messages)

print(ret.content)
```
    æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾åº¦ç ”å‘çš„çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œä¸­æ–‡åæ˜¯æ–‡å¿ƒä¸€è¨€ï¼Œè‹±æ–‡åæ˜¯ERNIE Botã€‚æˆ‘èƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚

    å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚


### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º

<img src="/images/ai/model_io.jpg" style="margin-left: 0px" width=500px>

### 1.2.1 Prompt æ¨¡æ¿å°è£…

1. PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡



```python
from langchain.prompts import PromptTemplate

template = PromptTemplate.from_template("ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯")
print("===Template===")
print(template)
print("===Prompt===")
print(template.format(subject='å°æ˜'))
```

    ===Template===
    input_variables=['subject'] template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'
    ===Prompt===
    ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯


2. ChatPromptTemplate ç”¨æ¨¡æ¿è¡¨ç¤ºçš„å¯¹è¯ä¸Šä¸‹æ–‡



```python
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain_openai import ChatOpenAI

template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(
            "ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}"),
        HumanMessagePromptTemplate.from_template("{query}"),
    ]
)

llm = ChatOpenAI()
prompt = template.format_messages(
    product="AGIè¯¾å ‚",
    name="ç“œç“œ",
    query="ä½ æ˜¯è°"
)

ret = llm.invoke(prompt)

print(ret.content)
```

    ä½ å¥½ï¼Œæˆ‘æ˜¯AGIè¯¾å ‚çš„å®¢æœåŠ©æ‰‹ï¼Œåå­—å«ç“œç“œã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£å†³å‘¢ï¼Ÿ


3. MessagesPlaceholder æŠŠå¤šè½®å¯¹è¯å˜æˆæ¨¡æ¿



```python
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)

human_prompt = "Translate your answer to {language}."
human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)

chat_prompt = ChatPromptTemplate.from_messages(
    # variable_name æ˜¯ message placeholder åœ¨æ¨¡æ¿ä¸­çš„å˜é‡å
    # ç”¨äºåœ¨èµ‹å€¼æ—¶ä½¿ç”¨
    [MessagesPlaceholder(variable_name="conversation"), human_message_template]
)
```


```python
from langchain_core.messages import AIMessage, HumanMessage

human_message = HumanMessage(content="Who is Elon Musk?")
ai_message = AIMessage(
    content="Elon Musk is a billionaire entrepreneur, inventor, and industrial designer"
)

messages = chat_prompt.format_prompt(
    # å¯¹ "conversation" å’Œ "language" èµ‹å€¼
    conversation=[human_message, ai_message], language="æ—¥æ–‡"
)

print(messages.to_messages())
```

    [HumanMessage(content='Who is Elon Musk?'), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer'), HumanMessage(content='Translate your answer to æ—¥æ–‡.')]



```python
result = llm.invoke(messages)
print(result.content)
```

    ã‚¤ãƒ¼ãƒ­ãƒ³ãƒ»ãƒã‚¹ã‚¯ã¯ã€å„„ä¸‡é•·è€…ã®èµ·æ¥­å®¶ã€ç™ºæ˜å®¶ã€ç”£æ¥­ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã§ã™ã€‚


<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°ï¼Œå¯ç±»æ¯”äº SK çš„ Semantic Function
</div>


### 1.2.2ã€ä»æ–‡ä»¶åŠ è½½ Prompt æ¨¡æ¿



```python
from langchain.prompts import PromptTemplate

template = PromptTemplate.from_file("example_prompt_template.txt")
print("===Template===")
print(template)
print("===Prompt===")
print(template.format(topic='é»‘è‰²å¹½é»˜'))
```

    ===Template===
    input_variables=['topic'] template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'
    ===Prompt===
    ä¸¾ä¸€ä¸ªå…³äºé»‘è‰²å¹½é»˜çš„ä¾‹å­


### 1.3 è¾“å‡ºå°è£… OutputParser

è‡ªåŠ¨æŠŠ LLM è¾“å‡ºçš„å­—ç¬¦ä¸²æŒ‰æŒ‡å®šæ ¼å¼åŠ è½½ã€‚

LangChain å†…ç½®çš„ OutputParser åŒ…æ‹¬:

- ListParser
- DatetimeParser
- EnumParser
- JsonOutputParser
- PydanticParser
- XMLParser

ç­‰ç­‰

### 1.3.1 Pydantic (JSON) Parser

è‡ªåŠ¨æ ¹æ® Pydantic ç±»çš„å®šä¹‰ï¼Œç”Ÿæˆè¾“å‡ºçš„æ ¼å¼è¯´æ˜



```python
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from typing import List, Dict

# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡


class Date(BaseModel):
    year: int = Field(description="Year")
    month: int = Field(description="Month")
    day: int = Field(description="Day")
    era: str = Field(description="BC or AD")

    # ----- å¯é€‰æœºåˆ¶ --------
    # ä½ å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ ¡éªŒæœºåˆ¶
    @validator('month')
    def valid_month(cls, field):
        if field <= 0 or field > 12:
            raise ValueError("æœˆä»½å¿…é¡»åœ¨1-12ä¹‹é—´")
        return field

    @validator('day')
    def valid_day(cls, field):
        if field <= 0 or field > 31:
            raise ValueError("æ—¥æœŸå¿…é¡»åœ¨1-31æ—¥ä¹‹é—´")
        return field

    @validator('day', pre=True, always=True)
    def valid_date(cls, day, values):
        year = values.get('year')
        month = values.get('month')

        # ç¡®ä¿å¹´ä»½å’Œæœˆä»½éƒ½å·²ç»æä¾›
        if year is None or month is None:
            return day  # æ— æ³•éªŒè¯æ—¥æœŸï¼Œå› ä¸ºæ²¡æœ‰å¹´ä»½å’Œæœˆä»½

        # æ£€æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆ
        if month == 2:
            if cls.is_leap_year(year) and day > 29:
                raise ValueError("é—°å¹´2æœˆæœ€å¤šæœ‰29å¤©")
            elif not cls.is_leap_year(year) and day > 28:
                raise ValueError("éé—°å¹´2æœˆæœ€å¤šæœ‰28å¤©")
        elif month in [4, 6, 9, 11] and day > 30:
            raise ValueError(f"{month}æœˆæœ€å¤šæœ‰30å¤©")

        return day

    @staticmethod
    def is_leap_year(year):
        if year % 400 == 0 or (year % 4 == 0 and year % 100 != 0):
            return True
        return False
```


```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI

from langchain_core.output_parsers import PydanticOutputParser


model_name = 'gpt-3.5-turbo'
temperature = 0
model = ChatOpenAI(model_name=model_name, temperature=temperature)

# æ ¹æ®Pydanticå¯¹è±¡çš„å®šä¹‰ï¼Œæ„é€ ä¸€ä¸ªOutputParser
parser = PydanticOutputParser(pydantic_object=Date)

template = """æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚
{format_instructions}
ç”¨æˆ·è¾“å…¥:
{query}"""

prompt = PromptTemplate(
    template=template,
    input_variables=["query"],
    # ç›´æ¥ä»OutputParserä¸­è·å–è¾“å‡ºæè¿°ï¼Œå¹¶å¯¹æ¨¡æ¿çš„å˜é‡é¢„å…ˆèµ‹å€¼
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

print("====Format Instruction=====")
print(parser.get_format_instructions())


query = "2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´..."
model_input = prompt.format_prompt(query=query)

print("====Prompt=====")
print(model_input.to_string())

output = model.invoke(model_input.to_messages())
print("====æ¨¡å‹åŸå§‹è¾“å‡º=====")
print(output.content)
print("====Parseåçš„è¾“å‡º=====")
date = parser.parse(output.content)
print(date)
```

    ====Format Instruction=====
    The output should be formatted as a JSON instance that conforms to the JSON schema below.

    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

    Here is the output schema:
    ```
    {"properties": {"year": {"title": "Year", "description": "Year", "type": "integer"}, "month": {"title": "Month", "description": "Month", "type": "integer"}, "day": {"title": "Day", "description": "Day", "type": "integer"}, "era": {"title": "Era", "description": "BC or AD", "type": "string"}}, "required": ["year", "month", "day", "era"]}
    ```
    ====Prompt=====
    æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚
    The output should be formatted as a JSON instance that conforms to the JSON schema below.

    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

    Here is the output schema:
    ```
    {"properties": {"year": {"title": "Year", "description": "Year", "type": "integer"}, "month": {"title": "Month", "description": "Month", "type": "integer"}, "day": {"title": "Day", "description": "Day", "type": "integer"}, "era": {"title": "Era", "description": "BC or AD", "type": "string"}}, "required": ["year", "month", "day", "era"]}
    ```
    ç”¨æˆ·è¾“å…¥:
    2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...
    ====æ¨¡å‹åŸå§‹è¾“å‡º=====
    {"year": 2023, "month": 4, "day": 6, "era": "AD"}
    ====Parseåçš„è¾“å‡º=====
    year=2023 month=4 day=6 era='AD'


### 1.3.2 Auto-Fixing Parser

åˆ©ç”¨ LLM è‡ªåŠ¨æ ¹æ®è§£æå¼‚å¸¸ä¿®å¤å¹¶é‡æ–°è§£æ



```python
from langchain.output_parsers import OutputFixingParser

new_parser = OutputFixingParser.from_llm(
    parser=parser, llm=ChatOpenAI(model="gpt-3.5-turbo"))

# æˆ‘ä»¬æŠŠä¹‹å‰outputçš„æ ¼å¼æ”¹é”™
output = output.content.replace("4", "å››æœˆ")
print("===æ ¼å¼é”™è¯¯çš„Output===")
print(output)
try:
    date = parser.parse(output)
except Exception as e:
    print("===å‡ºç°å¼‚å¸¸===")
    print(e)

# ç”¨OutputFixingParserè‡ªåŠ¨ä¿®å¤å¹¶è§£æ
date = new_parser.parse(output)
print("===é‡æ–°è§£æç»“æœ===")
print(date.json())
```

    ===æ ¼å¼é”™è¯¯çš„Output===
    {"year": 2023, "month": å››æœˆ, "day": 6, "era": "AD"}
    ===å‡ºç°å¼‚å¸¸===
    Invalid json output: {"year": 2023, "month": å››æœˆ, "day": 6, "era": "AD"}
    ===é‡æ–°è§£æç»“æœ===
    {"year": 2023, "month": 4, "day": 6, "era": "AD"}


<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b>çŒœä¸€ä¸‹OutputFixingParseræ˜¯æ€ä¹ˆåšåˆ°çš„
</div>


### 1.4ã€å°ç»“

1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§
2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿
3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡ï¼›é¢å¤–å¸¦æœ‰è‡ªåŠ¨ä¿®å¤åŠŸèƒ½ã€‚
4. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºä¼˜ç§€çš„éƒ¨åˆ†ï¼›ç¾ä¸­ä¸è¶³çš„æ˜¯ OutputParser è‡ªèº«çš„ Prompt ç»´æŠ¤åœ¨ä»£ç ä¸­ï¼Œè€¦åˆåº¦è¾ƒé«˜ã€‚


## äºŒã€æ•°æ®è¿æ¥å°è£…

<img src="/images/ai/data_connection.jpg" style="margin-left: 0px" width=500px>

### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders



```python
#!pip install pypdf
```


```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("llama2.pdf")
pages = loader.load_and_split()

print(pages[0].page_content)
```

    Llama 2 : Open Foundation and Fine-Tuned Chat Models
    Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ 
    Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
    Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
    Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
    Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
    Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
    Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
    Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
    Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
    Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
    Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
    Sergey Edunov Thomas Scialomâˆ—
    GenAI, Meta
    Abstract
    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
    Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
    models outperform open-source chat models on most benchmarks we tested, and based on
    ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
    source models. We provide a detailed description of our approach to fine-tuning and safety
    improvements of Llama 2-Chat in order to enable the community to build on our work and
    contribute to the responsible development of LLMs.
    âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
    â€ Second author
    Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023


### 2.2 æ–‡æ¡£å¤„ç†å™¨

### 2.2.1 TextSplitter



```python
#!pip install --upgrade langchain-text-splitters
```


```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=100,  # æ€è€ƒï¼šä¸ºä»€ä¹ˆè¦åšoverlap
    length_function=len,
    add_start_index=True,
)

paragraphs = text_splitter.create_documents([pages[0].page_content])
for para in paragraphs:
    print(para.page_content)
    print('-------')
```

    Llama 2 : Open Foundation and Fine-Tuned Chat Models
    Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ 
    Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
    -------
    Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
    Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
    -------
    Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
    Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
    -------
    Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
    Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
    -------
    Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
    Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
    -------
    Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
    Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
    -------
    Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
    Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
    -------
    Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
    Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
    -------
    Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
    -------
    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
    Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
    -------
    Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
    Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
    Sergey Edunov Thomas Scialomâˆ—
    -------
    Sergey Edunov Thomas Scialomâˆ—
    GenAI, Meta
    Abstract
    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
    -------
    Abstract
    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
    -------
    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
    Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
    -------
    Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
    models outperform open-source chat models on most benchmarks we tested, and based on
    -------
    models outperform open-source chat models on most benchmarks we tested, and based on
    ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
    -------
    ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
    source models. We provide a detailed description of our approach to fine-tuning and safety
    -------
    source models. We provide a detailed description of our approach to fine-tuning and safety
    improvements of Llama 2-Chat in order to enable the community to build on our work and
    -------
    improvements of Llama 2-Chat in order to enable the community to build on our work and
    contribute to the responsible development of LLMs.
    -------
    contribute to the responsible development of LLMs.
    âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
    â€ Second author
    -------
    âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
    â€ Second author
    Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023
    -------


<div class="alert alert-danger">
LangChain çš„ PDFLoader å’Œ TextSplitter å®ç°éƒ½æ¯”è¾ƒç²—ç³™ï¼Œå®é™…ç”Ÿäº§ä¸­ä¸å»ºè®®ä½¿ç”¨ã€‚
</div>


### 2.3ã€å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢



```python
#!pip install chromadb
__import__('pysqlite3')
import sys

sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
```


```python
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("llama2.pdf")
pages = loader.load_and_split()

# æ–‡æ¡£åˆ‡åˆ†
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=100,
    length_function=len,
    add_start_index=True,
)

texts = text_splitter.create_documents(
    [pages[2].page_content, pages[3].page_content])

# çŒåº“
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(texts, embeddings)

# æ£€ç´¢ top-1 ç»“æœ
retriever = db.as_retriever(search_kwargs={"k": 1})

docs = retriever.get_relevant_documents("llama 2æœ‰å¤šå°‘å‚æ•°ï¼Ÿ")

print(docs[0].page_content)
```

    but are not releasing.Â§
    2.Llama 2-Chat , a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
    variants of this model with 7B, 13B, and 70B parameters as well.
    WebelievethattheopenreleaseofLLMs,whendonesafely,willbeanetbenefittosociety. LikeallLLMs,


æ›´å¤šçš„ä¸‰æ–¹æ£€ç´¢ç»„ä»¶é“¾æ¥ï¼Œå‚è€ƒï¼šhttps://python.langchain.com/docs/integrations/vectorstores


### 2.4ã€å°ç»“

1. æ–‡æ¡£å¤„ç†éƒ¨åˆ† LangChain å®ç°è¾ƒä¸ºç²—ç³™ï¼Œå®é™…ç”Ÿäº§ä¸­ä¸å»ºè®®ä½¿ç”¨
2. ä¸å‘é‡æ•°æ®åº“çš„é“¾æ¥éƒ¨åˆ†æœ¬è´¨æ—¶æ¥å£å°è£…ï¼Œå‘é‡æ•°æ®åº“éœ€è¦è‡ªå·±é€‰å‹


## ä¸‰ã€è®°å¿†å°è£…ï¼šMemory

### 3.1ã€å¯¹è¯ä¸Šä¸‹æ–‡ï¼šConversationBufferMemory



```python
from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory

history = ConversationBufferMemory()
history.save_context({"input": "ä½ å¥½å•Š"}, {"output": "ä½ ä¹Ÿå¥½å•Š"})

print(history.load_memory_variables({}))

history.save_context({"input": "ä½ å†å¥½å•Š"}, {"output": "ä½ åˆå¥½å•Š"})

print(history.load_memory_variables({}))
```

    {'history': 'Human: ä½ å¥½å•Š\nAI: ä½ ä¹Ÿå¥½å•Š'}
    {'history': 'Human: ä½ å¥½å•Š\nAI: ä½ ä¹Ÿå¥½å•Š\nHuman: ä½ å†å¥½å•Š\nAI: ä½ åˆå¥½å•Š'}


### 3.2ã€åªä¿ç•™ä¸€ä¸ªçª—å£çš„ä¸Šä¸‹æ–‡ï¼šConversationBufferWindowMemory



```python
from langchain.memory import ConversationBufferWindowMemory

window = ConversationBufferWindowMemory(k=2)
window.save_context({"input": "ç¬¬ä¸€è½®é—®"}, {"output": "ç¬¬ä¸€è½®ç­”"})
window.save_context({"input": "ç¬¬äºŒè½®é—®"}, {"output": "ç¬¬äºŒè½®ç­”"})
window.save_context({"input": "ç¬¬ä¸‰è½®é—®"}, {"output": "ç¬¬ä¸‰è½®ç­”"})
print(window.load_memory_variables({}))
```

    {'history': 'Human: ç¬¬äºŒè½®é—®\nAI: ç¬¬äºŒè½®ç­”\nHuman: ç¬¬ä¸‰è½®é—®\nAI: ç¬¬ä¸‰è½®ç­”'}


### 3.3ã€é€šè¿‡ Token æ•°æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼šConversationTokenBufferMemory



```python
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import ChatOpenAI

memory = ConversationTokenBufferMemory(
    llm=ChatOpenAI(),
    max_token_limit=40
)
memory.save_context(
    {"input": "ä½ å¥½å•Š"}, {"output": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚"})
memory.save_context(
    {"input": "ä½ ä¼šå¹²ä»€ä¹ˆ"}, {"output": "æˆ‘ä»€ä¹ˆéƒ½ä¼š"})

print(memory.load_memory_variables({}))
```

    {'history': 'AI: ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚\nHuman: ä½ ä¼šå¹²ä»€ä¹ˆ\nAI: æˆ‘ä»€ä¹ˆéƒ½ä¼š'}


### 3.4ã€æ›´å¤šç±»å‹

- ConversationSummaryMemory: å¯¹ä¸Šä¸‹æ–‡åšæ‘˜è¦
  - https://python.langchain.com/docs/modules/memory/types/summary
- ConversationSummaryBufferMemory: ä¿å­˜ Token æ•°é™åˆ¶å†…çš„ä¸Šä¸‹æ–‡ï¼Œå¯¹æ›´æ—©çš„åšæ‘˜è¦
  - https://python.langchain.com/docs/modules/memory/types/summary_buffer
- VectorStoreRetrieverMemory: å°† Memory å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥æ£€ç´¢å›æœ€ç›¸å…³çš„éƒ¨åˆ†
  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory


### 3.5ã€å°ç»“

1. LangChain çš„ Memory ç®¡ç†æœºåˆ¶å±äºå¯ç”¨çš„éƒ¨åˆ†ï¼Œå°¤å…¶æ˜¯ç®€å•æƒ…å†µå¦‚æŒ‰è½®æ•°æˆ–æŒ‰ Token æ•°ç®¡ç†ï¼›
2. å¯¹äºå¤æ‚æƒ…å†µï¼Œå®ƒä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„å®ç°ï¼Œä¾‹å¦‚æ£€ç´¢å‘é‡åº“æ–¹å¼ï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µå’Œæ•ˆæœè¯„ä¼°ï¼›
3. ä½†æ˜¯**å®ƒå¯¹å†…å­˜çš„å„ç§ç»´æŠ¤æ–¹æ³•çš„æ€è·¯åœ¨å®é™…ç”Ÿäº§ä¸­å¯ä»¥å€Ÿé‰´**ã€‚


## å››ã€Chain å’Œ LangChain Expression Language (LCEL)


LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚

LCEL çš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š

1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä» LLM æµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚

2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚

3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚

4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚

5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚

6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚

7. **æ— ç¼ LangSmith è·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚

8. **æ— ç¼ LangServe éƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚

åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/


### 4.1 Pipeline å¼è°ƒç”¨ PromptTemplate, LLM å’Œ OutputParser



```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from typing import List, Dict, Optional
from enum import Enum
```


```python
# è¾“å‡ºç»“æ„
class SortEnum(str, Enum):
    data = 'data'
    price = 'price'


class OrderingEnum(str, Enum):
    ascend = 'ascend'
    descend = 'descend'


class Semantics(BaseModel):
    name: Optional[str] = Field(description="æµé‡åŒ…åç§°", default=None)
    price_lower: Optional[int] = Field(description="ä»·æ ¼ä¸‹é™", default=None)
    price_upper: Optional[int] = Field(description="ä»·æ ¼ä¸Šé™", default=None)
    data_lower: Optional[int] = Field(description="æµé‡ä¸‹é™", default=None)
    data_upper: Optional[int] = Field(description="æµé‡ä¸Šé™", default=None)
    sort_by: Optional[SortEnum] = Field(description="æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº", default=None)
    ordering: Optional[OrderingEnum] = Field(
        description="å‡åºæˆ–é™åºæ’åˆ—", default=None)


# OutputParser
parser = PydanticOutputParser(pydantic_object=Semantics)

# Prompt æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\n{format_instructions}\nä¸è¦è¾“å‡ºæœªæåŠçš„å­—æ®µã€‚",
        ),
        ("human", "{text}"),
    ]
).partial(format_instructions=parser.get_format_instructions())

# æ¨¡å‹
model = ChatOpenAI(model="gpt-4-0125-preview", temperature=0)

# LCEL è¡¨è¾¾å¼
runnable = (
    {"text": RunnablePassthrough()} | prompt | model | parser
)

# è¿è¡Œ
ret = runnable.invoke("ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›")
print(ret.json())
```

    {"name": null, "price_lower": null, "price_upper": 100, "data_lower": null, "data_upper": null, "sort_by": "data", "ordering": "descend"}


<div class="alert alert-warning">
<b>æ³¨æ„:</b> åœ¨å½“å‰çš„æ–‡æ¡£ä¸­ LCEL äº§ç”Ÿçš„å¯¹è±¡ï¼Œè¢«å«åš runnable æˆ– chainï¼Œç»å¸¸ä¸¤ç§å«æ³•æ··ç”¨ã€‚æœ¬è´¨å°±æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰è°ƒç”¨æµç¨‹ã€‚
</div>


<div class="alert alert-success">
<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />
å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼šhttps://python.langchain.com/docs/expression_language/why
</div>


### 4.2 ç”¨ LCEL å®ç° RAG



```python
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("llama2.pdf")
pages = loader.load_and_split()

# æ–‡æ¡£åˆ‡åˆ†
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=100,
    length_function=len,
    add_start_index=True,
)

texts = text_splitter.create_documents(
    [pages[2].page_content, pages[3].page_content])

# çŒåº“
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(texts, embeddings)

# æ£€ç´¢ top-1 ç»“æœ
retriever = db.as_retriever(search_kwargs={"k": 1})
```


```python
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

# Promptæ¨¡æ¿
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# Chain
rag_chain = (
    {"question": RunnablePassthrough(), "context": retriever}
    | prompt
    | model
    | StrOutputParser()
)

rag_chain.invoke("Llama 2æœ‰å¤šå°‘å‚æ•°")
```




    'Llama 2æœ‰7Bã€13Bå’Œ70Bå‚æ•°çš„ç‰ˆæœ¬ã€‚'



### 4.3 é€šè¿‡ LCEL å®ç° Function Calling



```python
from langchain_core.tools import tool


@tool
def multiply(first_int: int, second_int: int) -> int:
    """ä¸¤ä¸ªæ•´æ•°ç›¸ä¹˜"""
    return first_int * second_int


@tool
def add(first_int: int, second_int: int) -> int:
    "Add two integers."
    return first_int + second_int


@tool
def exponentiate(base: int, exponent: int) -> int:
    "Exponentiate the base to the exponent power."
    return base**exponent
```


```python
from langchain_core.output_parsers import StrOutputParser
from langchain.output_parsers import JsonOutputToolsParser

tools = [multiply, add, exponentiate]
# å¸¦æœ‰åˆ†æ”¯çš„ LCEL
llm_with_tools = llm.bind_tools(tools) | {
    "functions": JsonOutputToolsParser(),
    "text": StrOutputParser()
}
```


```python
result = llm_with_tools.invoke("1024çš„16å€æ˜¯å¤šå°‘")

print(result)
```

    {'functions': [{'type': 'multiply', 'args': {'first_int': 1024, 'second_int': 16}}], 'text': ''}



```python
result = llm_with_tools.invoke("ä½ æ˜¯è°")

print(result)
```

    {'functions': [], 'text': 'æˆ‘æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©æ‚¨è¿›è¡Œæ•°å­¦è®¡ç®—å’Œæ‰§è¡Œå„ç§ä»»åŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ'}


#### ç›´æ¥é€‰æ‹©å·¥å…·å¹¶è¿è¡Œï¼ˆé€‰ï¼‰



```python
from typing import Union
from operator import itemgetter
from langchain_core.runnables import (
    Runnable,
    RunnableLambda,
    RunnableMap,
    RunnablePassthrough,
)

# åç§°åˆ°å‡½æ•°çš„æ˜ å°„
tool_map = {tool.name: tool for tool in tools}


def call_tool(tool_invocation: dict) -> Union[str, Runnable]:
    """Function for dynamically constructing the end of the chain based on the model-selected tool."""
    tool = tool_map[tool_invocation["type"]]
    return RunnablePassthrough.assign(
        output=itemgetter("args") | tool
    )


# .map() allows us to apply a function to a list of inputs.
call_tool_list = RunnableLambda(call_tool).map()
```


```python
import json


def route(response):
    if len(response["functions"]) > 0:
        return response["functions"]
    else:
        return response["text"]


llm_with_tools = llm.bind_tools(tools) | {
    "functions": JsonOutputToolsParser() | call_tool_list,
    "text": StrOutputParser()
} | RunnableLambda(route)

result = llm_with_tools.invoke("1024çš„å¹³æ–¹æ˜¯å¤šå°‘")
print(result)

result = llm_with_tools.invoke("ä½ å¥½")
print(result)
```

    [{'type': 'exponentiate', 'args': {'base': 1024, 'exponent': 2}, 'output': 1048576}]
    ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ


<div class="alert alert-warning">
è¿™ç§å†™æ³•å¯è¯»æ€§å¤ªå·®äº†ï¼Œä¸ªäººä¸å»ºè®®ä½¿ç”¨è¿‡äºå¤æ‚çš„ LCELï¼
</div>


<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
</div>


### é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°

1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/docs/expression_language/how_to/configure
2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/docs/expression_language/how_to/fallbacks
3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/docs/expression_language/how_to/map
4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/docs/expression_language/how_to/routing
5. è°ƒç”¨è‡ªå®šä¹‰æµå¼å‡½æ•°ï¼šhttps://python.langchain.com/docs/expression_language/how_to/generators
6. é“¾æ¥å¤–éƒ¨ Memoryï¼šhttps://python.langchain.com/docs/expression_language/how_to/message_history

æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/docs/expression_language/cookbook/


## äº”ã€æ™ºèƒ½ä½“æ¶æ„ï¼šAgent


### 5.1 å›å¿†ï¼šä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰

å°†å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ä¸ªæ¨ç†å¼•æ“ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨ç”Ÿæˆå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤ï¼Œæ‰§è¡Œç›¸åº”åŠ¨ä½œï¼ˆä¾‹å¦‚é€‰æ‹©å¹¶è°ƒç”¨å·¥å…·ï¼‰ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚

<img src="/images/ai/agent-overview.png" style="margin-left: 0px" width=500px>


### 5.2 å…ˆå®šä¹‰ä¸€äº›å·¥å…·ï¼šTools

- å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°æˆ–ä¸‰æ–¹ API
- ä¹Ÿå¯ä»¥æŠŠä¸€ä¸ª Chain æˆ–è€… Agent çš„ run()ä½œä¸ºä¸€ä¸ª Tool



```python
from langchain_community.utilities import SerpAPIWrapper
from langchain.tools import Tool, tool

search = SerpAPIWrapper()
tools = [
    Tool.from_function(
        func=search.run,
        name="Search",
        description="useful for when you need to answer questions about current events"
    ),
]
```


```python
import calendar
import dateutil.parser as parser
from datetime import date

# è‡ªå®šä¹‰å·¥å…·


@tool("weekday")
def weekday(date_str: str) -> str:
    """Convert date to weekday name"""
    d = parser.parse(date_str)
    return calendar.day_name[d.weekday()]


tools += [weekday]
```

### 5.3 æ™ºèƒ½ä½“ç±»å‹ï¼šReAct


<img src="/images/ai/ReAct.png" style="margin-left: 0px" width=500px>



```python
!pip install google-search-results
```


```python
!pip install --upgrade langchainhub
```


```python
from langchain import hub
import json

# ä¸‹è½½ä¸€ä¸ªç°æœ‰çš„ Prompt æ¨¡æ¿
prompt = hub.pull("hwchase17/react")

print(prompt.template)
```

    Answer the following questions as best you can. You have access to the following tools:

    {tools}

    Use the following format:

    Question: the input question you must answer
    Thought: you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action
    Observation: the result of the action
    ... (this Thought/Action/Action Input/Observation can repeat N times)
    Thought: I now know the final answer
    Final Answer: the final answer to the original input question

    Begin!

    Question: {input}
    Thought:{agent_scratchpad}



```python
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_react_agent


llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)

# å®šä¹‰ä¸€ä¸ª agent: éœ€è¦å¤§æ¨¡å‹ã€å·¥å…·é›†ã€å’Œ Prompt æ¨¡æ¿
agent = create_react_agent(llm, tools, prompt)
# å®šä¹‰ä¸€ä¸ªæ‰§è¡Œå™¨ï¼šéœ€è¦ agent å¯¹è±¡ å’Œ å·¥å…·é›†
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# æ‰§è¡Œ
agent_executor.invoke({"input": "å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå‡ "})
```



    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mæˆ‘éœ€è¦çŸ¥é“å‘¨æ°ä¼¦çš„å‡ºç”Ÿæ—¥æœŸï¼Œç„¶åæˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚
    Action: Search
    Action Input: å‘¨æ°ä¼¦çš„å‡ºç”Ÿæ—¥æœŸ[0m[36;1m[1;3mJanuary 18, 1979 (age 45 years), Linkou District, Taipei, Taiwan[0m[32;1m[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„å‡ºç”Ÿæ—¥æœŸæ˜¯1979å¹´1æœˆ18æ—¥ã€‚æˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚
    Action: weekday
    Action Input: 1979-01-18[0m[33;1m[1;3mThursday[0m[32;1m[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚
    Final Answer: å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚[0m

    [1m> Finished chain.[0m





    {'input': 'å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ', 'output': 'å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚'}



### 5.4 æ™ºèƒ½ä½“ç±»å‹ï¼šSelfAskWithSearch



```python
# ä¸‹è½½ä¸€ä¸ªæ¨¡æ¿
prompt = hub.pull("hwchase17/self-ask-with-search")

print(prompt.template)
```

    Question: Who lived longer, Muhammad Ali or Alan Turing?
    Are follow up questions needed here: Yes.
    Follow up: How old was Muhammad Ali when he died?
    Intermediate answer: Muhammad Ali was 74 years old when he died.
    Follow up: How old was Alan Turing when he died?
    Intermediate answer: Alan Turing was 41 years old when he died.
    So the final answer is: Muhammad Ali

    Question: When was the founder of craigslist born?
    Are follow up questions needed here: Yes.
    Follow up: Who was the founder of craigslist?
    Intermediate answer: Craigslist was founded by Craig Newmark.
    Follow up: When was Craig Newmark born?
    Intermediate answer: Craig Newmark was born on December 6, 1952.
    So the final answer is: December 6, 1952

    Question: Who was the maternal grandfather of George Washington?
    Are follow up questions needed here: Yes.
    Follow up: Who was the mother of George Washington?
    Intermediate answer: The mother of George Washington was Mary Ball Washington.
    Follow up: Who was the father of Mary Ball Washington?
    Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
    So the final answer is: Joseph Ball

    Question: Are both the directors of Jaws and Casino Royale from the same country?
    Are follow up questions needed here: Yes.
    Follow up: Who is the director of Jaws?
    Intermediate answer: The director of Jaws is Steven Spielberg.
    Follow up: Where is Steven Spielberg from?
    Intermediate answer: The United States.
    Follow up: Who is the director of Casino Royale?
    Intermediate answer: The director of Casino Royale is Martin Campbell.
    Follow up: Where is Martin Campbell from?
    Intermediate answer: New Zealand.
    So the final answer is: No

    Question: {input}
    Are followup questions needed here:{agent_scratchpad}



```python
from langchain.agents import create_self_ask_with_search_agent

tools = [
    Tool(
        name="Intermediate Answer",
        func=search.run,
        description="useful for when you need to ask with search.",
    )
]

# self_ask_with_search_agent åªèƒ½ä¼ ä¸€ä¸ªåä¸º 'Intermediate Answer' çš„ tool
agent = create_self_ask_with_search_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

agent_executor.invoke({"input": "å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®"})
```



    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mYes.
    Follow up: Who is å´äº¬'s wife?[0m[36;1m[1;3m['ç®€ä»‹ï¼š ä½ çŸ¥é“å´äº¬å¨¶è¿‡å‡ ä¸ªè€å©†å—åªç»“äº†ä¸€æ¬¡å©šç›®å‰ä»–æœ‰ä¸¤ä¸ªå­©å­å´äº¬çš„è€å©†æ˜¯è°¢æ¥ ä»–ä»¬æ˜¯åœ¨2014å¹´ç»“çš„å©š2018å¹´ç”Ÿçš„ç¬¬äºŒä¸ªå­©å­å´è™‘1974å¹´4... å°æå­çœŸå®å½±åƒ. 2022å¹´8æœˆ10æ—¥.', 'å´äº¬æœ‰å‡ ä»»è€å©†- ç™¾åº¦ å´äº¬\ue641åªæœ‰ä¸€ä»»è€å©† ï¼Œå¥¹å°±æ˜¯è°¢æ¥ \ue641ã€‚ å´äº¬å’Œè°¢æ¥ åœ¨2014å¹´ç»“å©šï¼Œè‚²æœ‰ä¸¤ä¸ªå„¿å­ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ‰äº›ç½‘ç»œä¼ é—»å¯èƒ½ä¼šé”™è¯¯åœ°å£°ç§°å´äº¬æœ‰å¤šä½å¦»å­ï¼Œä½†è¿™äº›éƒ½ä¸æ˜¯äº‹å®ã€‚', 'å´äº¬å¦»å­è°¢æ¥ ï¼Œå†²ä¸Šçƒ­æœ!7æœˆ14æ—¥ï¼Œå´äº¬å¦»å­åœ¨ä¸ªäººç¤¾äº¤å¹³å°å‘äº†ä¸€ç»„ç…§ç‰‡ï¼Œå¼•èµ·çƒ­è®®ã€‚ç…§ç‰‡ä¸­ï¼Œå¥¹çš„å¤´å‘å¤„åœ¨å°´å°¬æœŸï¼Œå’Œå¾€æ—¥å¹²ç»ƒåˆ©è½çš„å½¢è±¡å¤§ç›¸å¾„åº­ã€‚', 'ä¸»æ¼”ã€Œæˆ˜ç‹¼ã€ç³»åˆ—ç”µå½±æš´çº¢çš„ä¸­å›½ç”·æ˜Ÿå´äº¬ï¼Œå’Œå°9å²çš„å¦»å­è°¢æ¥ ç»“å©šå¤šå¹´æ¥ï¼Œæ„Ÿæƒ…ç”œèœœï¼Œæ˜¯åœˆå†…å…¬è®¤ç¥ä»™çœ·ä¾£ã€‚ è¿‘æ—¥ï¼Œæœ‰ç½‘å‹ç¿»å‡ºä¸¤äººæ˜”æ—¥ä¸€èµ·ä¸ŠçœŸäººç§€ç»¼è‰ºï¼Œå´äº¬åœ¨èŠ‚ç›®ä¸­å‘è„¾æ°”çš„ç‰‡æ®µï¼Œå¹¶æŒ‡è´£ä»–æš´åŠ›ã€å¤§ç”·å­ä¸»ä¹‰ã€ä¸çˆ±è€å©†ï¼Œè¿˜åŠè°¢æ¥ ç¦»å©šã€‚ å¯¹æ­¤ï¼Œè°¢æ¥ å‘é•¿æ–‡è¯‰è¯´çœŸå®æ„Ÿæƒ³ã€‚', 'å´äº¬çš„è€å©† Â· è¿™å¤«å¦»ä¿©æ˜¯çœŸä¸æŠŠå„ä½æ‹†è…»ä¸å½“å¤–äººå•Šï¼ Â· å¦‚æœä½ çš„å¦ä¸€åŠæ˜¯å´äº¬ï¼Œä½ èƒ½æŠ—ä½ä»–è¿™ä¹ˆé€ å—ï¼Ÿ Â· #å´äº¬é€‰è€å©†çš„çœ¼å…‰ä¸å¾—ä¸æœå•Š #è°¢æ¥ é•¿å‘çœŸçš„å¥½ç¾ï¼Œå¤ª ...', '12æœˆ8æ—¥æ·±å¤œï¼Œè°¢æ¥ åˆ†äº«äº†å®¶é‡Œå¤§å…¬å­çš„èŒç…§ï¼Œå¹¶è¯´é“ï¼šä¼šåšç”œç‚¹çš„ç”·å­©å­ï¼Œåº”è¯¥èƒ½å«çš„å‡ºå»å§ï¼å­©å­è¿˜è¿™ä¹ˆå°ï¼Œè°¢æ¥ å·²ç»å¼€å§‹æ‹…å¿ƒå´æ‰€è°“çš„å©šå§»å¤§äº‹äº†â€¦â€¦', 'ä¸€å¼ å´äº¬å¦»å­è°¢æ¥ ä¸çŸ¥åå¥³æ¼”å‘˜è´¾ç²çš„åˆå½±åœ¨ç½‘ç»œä¸Šå¼•èµ·äº†çƒ­è®®ã€‚ ä¸¤ä½ç¾å¥³åŒæ¡†ï¼Œé¢œå€¼å–œäººï¼Œæ°”è´¨æ›´æ˜¯èˆ¬é…å¾—å¦‚åŒäº²å§å¦¹ä¸€èˆ¬ï¼Œè®©äººä¸ç¦èµå¹ä¸å·²ã€‚', 'ä¸»æ¼”ã€Œæˆ°ç‹¼ã€ç³»åˆ—é›»å½±æš´ç´…çš„ä¸­åœ‹ç”·æ˜Ÿå³äº¬ï¼Œå’Œå°9æ­²çš„å¦»å­è¬æ¥ çµå©šå¤šå¹´ä¾†ï¼Œæ„Ÿæƒ…ç”œèœœï¼Œæ˜¯åœˆå…§å…¬èªç¥ä»™çœ·ä¾¶ã€‚ è¿‘æ—¥ï¼Œæœ‰ç¶²å‹ç¿»å‡ºå…©äººæ˜”æ—¥ä¸€èµ·ä¸ŠçœŸäººç§€ç¶œè—ï¼Œå³äº¬åœ¨ç¯€ç›®ä¸­ç™¼è„¾æ°£çš„ç‰‡æ®µï¼Œä¸¦æŒ‡è²¬ä»–æš´åŠ›ã€å¤§ç”·å­ä¸»ç¾©ã€ä¸æ„›è€å©†ï¼Œé‚„å‹¸è¬æ¥ é›¢å©šã€‚ å°æ­¤ï¼Œè¬æ¥ ç™¼é•·æ–‡è¨´èªªçœŸå¯¦æ„Ÿæƒ³ã€‚', 'ä¸ä¹…åå´äº¬å‚åŠ ä¸€æ¡£è°¢æ¥ ä¸»æŒçš„èŠ‚ç›®ï¼Œé—®è°¢æ¥ ä½ æœ‰å¯¹è±¡å—ï¼Ÿ è°¢æ¥ è¯´æ²¡æœ‰ï¼Œå´äº¬å¼€å§‹è¿½æ±‚å¥¹ï¼Œä¸ºäº†èƒ½å¨¶å¾—ç¾äººå½’ï¼Œç‰¹åœ°å»è°¢æ¥ å®¶è®¨å¥½è°¢æ¥ çš„çˆ¶æ¯ï¼Œæœ€ç»ˆæ‰“åŠ¨è°¢æ¥ å«ç»™äº†ä»–ã€‚ å½“æ—¶è°¢æ¥ å·²ç»æ˜¯çŸ¥åä¸»æŒäººï¼Œè€Œå´äº¬è¿˜æ˜¯ä¸ªå°æ¼”å‘˜ï¼Œèƒ½å¨¶åˆ°è°¢æ¥ ä¸ºå´äº¬äº‰å›äº†è¢«ç”©çš„é¢å­ã€‚ å´äº¬æ‹ç…§ã€Šæˆ˜ç‹¼ã€‹çš„æ—¶å€™èµ„é‡‘ä¸å¤Ÿï¼Œè°¢æ¥ å–äº†è‡ªå·±ä¸€å¥—æˆ¿å¸® ...'][0m[32;1m[1;3mFollow up: What variety shows has è°¢æ¥  hosted?[0m[36;1m[1;3m['TV Show ; 2024, Love Actually Season 3 add. Chinese TV Show, 2024, 10 eps. (Main Host). 10 ; 2023, Ace vs Ace Season 8 add. Chinese TV Show, 2023, 13 eps. (Ep. 1) ...', 'ç°ä¸ºå…‰çº¿ä¼ åª’æ——ä¸‹ä¸»æ‰“èŠ‚ç›®ã€Šå¨±ä¹ç°åœºã€‹ã€ã€Šæœ€ä½³ç°åœºã€‹ã€ã€Šå½±è§†é£äº‘æ¦œã€‹å½“å®¶ä¸»æŒï¼Œæ›´æ˜¯é»‘é¾™æ±Ÿå«è§†ã€Šå¿«æ´»æ­¦æ—ã€‹ã€æ·±åœ³å«è§†ã€Šå¤§ç‰Œç”Ÿæ—¥ä¼šã€‹ã€è¾½å®å«è§†ã€Šè°æ˜¯ä¸»è§’ã€‹ç­‰èŠ‚ç›®å¥³ä¸»æŒã€‚', 'è°¢æ¥ ï¼Œ1983å¹´11æœˆ6æ—¥å‡ºç”Ÿäºå®‰å¾½çœåˆè‚¥å¸‚ï¼Œä¸­å›½å†…åœ°èŠ‚ç›®å¥³ä¸»æŒäººã€å½±è§†æ¼”å‘˜ï¼Œæ¯•ä¸šäºå®‰å¾½è´¢ç»å¤§å­¦ã€‚2005å¹´ï¼Œåœ¨â€œé­…åŠ›ä¸»æŒå¤§èµ›â€ä¸Šè·å¾—å† å†›å¹¶ç­¾çº¦å…‰çº¿ä¼ åª’ï¼Œä»è€Œå¼€å¯äº†å¥¹çš„ ...', 'ã€æ¹–å—å«è§†å¿«ä¹å¤§æœ¬è¥-æœ¬æœŸç²¾å½©ã€‘å¼ æ°ã€è°­ç»´ç»´å°†åšå®¢ã€Šå¿«ä¹å¤§æœ¬è¥ã€‹ï¼Œå’Œå…‰çº¿ä¸»æ’­è°¢æ¥ ã€æ²ˆå‡Œä¸€èµ·åˆ†äº«æ—…è¡Œè¶£äº‹ã€‚æœ¬æœŸèŠ‚ç›®ï¼Œå˜‰å®¾ä»¬è¦å±•ç¤ºæ—…è¡Œæ—¶ä¹±ä¹°çš„ ...', 'Hi Everyone, welcome to Variety show Here will provide the latest ... è°¢æ¥ è¿™æ®µè¯å¤ªæœºæ™ºï¼Œç»™è¶³å´äº¬é¢å­ï¼Œéš¾æ€ªæˆ˜ç‹¼ ... Variety show. 307K. Subscribe.', "']Follow up: What variety shows has è°¢æ¥ hosted?['TBA, Love Actually Season 3 add. Chinese TV Show, 0000, 10 eps. (Main Host). 10 ; 2023, Ace ...", 'The emergence of mainstreaming resistance shows that intellectual culture has not disappeared from Chinese TV drama. And even though mainstreaming ...', '/www.youtube.com/playlist?list=PLua-L-o3nGJr8hKzBK7zDrZSwf4VgvWI5 æ¯å¤©ç²¾å½©ä¸æ–·æ›´æ–°ï¼ Thanks for watchingï¼ Hi Everyone, welcome to Variety show ...', 'The image shows the " NTU-NPM Joint Project ... å»·èŒ„æ¥  ... The NPM\'s Southern Branch has been hosting the â€œNPM Asian Art Festivalâ€ every year since.'][0m[32;1m[1;3mSo the final answer is: Love Actually Season 3, Ace vs Ace Season 8, å¨±ä¹ç°åœº, æœ€ä½³ç°åœº, å½±è§†é£äº‘æ¦œ, å¿«æ´»æ­¦æ—, å¤§ç‰Œç”Ÿæ—¥ä¼š, è°æ˜¯ä¸»è§’, å¿«ä¹å¤§æœ¬è¥.[0m

    [1m> Finished chain.[0m





    {'input': 'å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®',
     'output': 'Love Actually Season 3, Ace vs Ace Season 8, å¨±ä¹ç°åœº, æœ€ä½³ç°åœº, å½±è§†é£äº‘æ¦œ, å¿«æ´»æ­¦æ—, å¤§ç‰Œç”Ÿæ—¥ä¼š, è°æ˜¯ä¸»è§’, å¿«ä¹å¤§æœ¬è¥.'}



<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>
<ol>
<li>ReAct æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ Planner</li>
<li>SelfAskWithSearch æ›´é€‚åˆéœ€è¦å±‚å±‚æ¨ç†çš„åœºæ™¯ï¼ˆä¾‹å¦‚çŸ¥è¯†å›¾è°±ï¼‰</li>
<li>Agentè½åœ°åº”ç”¨éœ€è¦æ›´å¤šç»†èŠ‚ï¼Œåé¢è¯¾ç¨‹ä¸­æˆ‘ä»¬ä¼šä¸“é—¨è®² Agent çš„å®ç°</li>
</ol>
</div>


## å…­ã€LangServe

LangServe ç”¨äºå°† Chain æˆ–è€… Runnable éƒ¨ç½²æˆä¸€ä¸ª REST API æœåŠ¡ã€‚



```python
# å®‰è£… LangServe
!pip install "langserve[all]"

# ä¹Ÿå¯ä»¥åªå®‰è£…ä¸€ç«¯
# !pip install "langserve[client]"
# !pip install "langserve[server]"
```

### 6.1ã€Server ç«¯


```python
#!/usr/bin/env python
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langserve import add_routes
import uvicorn

app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple api server using Langchain's Runnable interfaces",
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
add_routes(
    app,
    prompt | model,
    path="/joke",
)

if __name__ == "__main__":
    uvicorn.run(app, host="localhost", port=9999)
```


### 6.2ã€Client ç«¯


```python
import requests

response = requests.post(
    "http://localhost:9999/joke/invoke",
    json={'input': {'topic': 'å°æ˜'}}
)
print(response.json())
```


## ä¸ƒã€LangChain.js

Python ç‰ˆ LangChain çš„å§Šå¦¹é¡¹ç›®ï¼Œéƒ½æ˜¯ç”± Harrison Chase ä¸»ç†ã€‚

é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai/langchainjs

æ–‡æ¡£åœ°å€ï¼šhttps://js.langchain.com/docs/

ç‰¹è‰²ï¼š

1. å¯ä»¥å’Œ Python ç‰ˆ LangChain æ— ç¼å¯¹æ¥

2. æŠ½è±¡è®¾è®¡å®Œå…¨ç›¸åŒï¼Œæ¦‚å¿µä¸€ä¸€å¯¹åº”

3. æ‰€æœ‰å¯¹è±¡åºåˆ—åŒ–åéƒ½èƒ½è·¨è¯­è¨€ä½¿ç”¨ï¼Œä½† API å·®åˆ«æŒºå¤§ï¼Œä¸è¿‡åœ¨åŠªåŠ›å¯¹é½

æ”¯æŒç¯å¢ƒï¼š

1. Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x
2. Cloudflare Workers
3. Vercel / Next.js (Browser, Serverless and Edge functions)
4. Supabase Edge Functions
5. Browser
6. Deno

å®‰è£…ï¼š

```
npm install langchain
```

å½“å‰é‡ç‚¹ï¼š

1. è¿½ä¸Š Python ç‰ˆçš„èƒ½åŠ›ï¼ˆç”šè‡³ä¸ºæ­¤åšäº†ä¸€ä¸ªåŸºäº gpt-3.5-turbo çš„ä»£ç ç¿»è¯‘å™¨ï¼‰
2. ä¿æŒå…¼å®¹å°½å¯èƒ½å¤šçš„ç¯å¢ƒ
3. å¯¹è´¨é‡å…³æ³¨ä¸å¤šï¼Œéšæ—¶é—´è‡ªç„¶èƒ½è§£å†³


## LangChain ä¸ Semantic Kernel å¯¹æ¯”


| åŠŸèƒ½/å·¥å…·          |     LangChain      |       Semantic Kernel       |
| ------------------ | :----------------: | :-------------------------: |
| ç‰ˆæœ¬å·             |       0.1.12       |       python-0.9.2b1        |
| é€‚é…çš„ LLM         |         å¤š         |        å°‘ + å¤–éƒ¨ç”Ÿæ€        |
| Prompt å·¥å…·        |        æ”¯æŒ        |            æ”¯æŒ             |
| Prompt å‡½æ•°åµŒå¥—    |   éœ€è¦é€šè¿‡ LCEL    |            æ”¯æŒ             |
| Prompt æ¨¡æ¿åµŒå¥—    |        æ”¯æŒ        |           ä¸æ”¯æŒ            |
| è¾“å‡ºè§£æå·¥å…·       |        æ”¯æŒ        |           ä¸æ”¯æŒ            |
| ä¸Šä¸‹æ–‡ç®¡ç†å·¥å…·     |        æ”¯æŒ        | C#ç‰ˆæ”¯æŒï¼ŒPython ç‰ˆå°šæœªæ”¯æŒ |
| å†…ç½®å·¥å…·           |   å¤šï¼Œä½†è‰¯è ä¸é½   |        å°‘ + å¤–éƒ¨ç”Ÿæ€        |
| ä¸‰æ–¹å‘é‡æ•°æ®åº“é€‚é… |         å¤š         |        å°‘ + å¤–éƒ¨ç”Ÿæ€        |
| æœåŠ¡éƒ¨ç½²           |     LangServe      |     ä¸ Azure è¡”æ¥æ›´ä¸æ»‘     |
| ç®¡ç†å·¥å…·           | LangSmith/LangFuse |         Prompt Flow         |


## æ€»ç»“

1. LangChain éšç€ç‰ˆæœ¬è¿­ä»£å¯ç”¨æ€§æœ‰æ˜æ˜¾æå‡
2. ä½¿ç”¨ LangChain è¦æ³¨æ„ç»´æŠ¤è‡ªå·±çš„ Promptï¼Œå°½é‡ Prompt ä¸ä»£ç é€»è¾‘è§£ä¾èµ–
3. å®ƒçš„å†…ç½®åŸºç¡€å·¥å…·ï¼Œå»ºè®®å……åˆ†æµ‹è¯•æ•ˆæœåå†å†³å®šæ˜¯å¦ä½¿ç”¨



